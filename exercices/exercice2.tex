\section*{exercise 2 (15.04.2016)}
\begin{enumerate}[(1)]
 \item likelihood Funktion: $L(\Theta) = \prod^m_{i=1} h_\Theta(x^{(i)}) (1-h_\Theta(x^{(i)}))^{1-y^{(i)}}$\\
 $h_\Theta(x) = \frac{1}{1+ \exp (- \Theta^T x)}$\\
 \textcolor{red}{ gesucht: $\Theta^k = \stackrel{argmax}{\Theta \in \mathbb{R}^n} l(\Theta) = \stackrel{argmax}{\Theta \in \mathbb{R}^n} \log L(\Theta) = \stackrel{argmax}{\Theta \in \mathbb{R}^n} \Sigma^m_{i=1} y^{(i)} \log h_\Theta(x^{(i)}) + (1-y^{(i)}) \log (1- h_\Theta(x^{(i)}))$}\\
 Ziel heute: Zeige, dass $l(\Theta)$ konkave Funktion ist, d.~h.
 \[l(\alpha \Theta_1 + (1- \alpha) \Theta_2) \geq \alpha l(\Theta_1) + (1-\alpha) l(\Theta_2)\]
\textcolor{red}{Es reicht zu zeigen, dass die Hesse-Matrix von $l(\Theta)$ negativ semidefinit ist.}

%bild
\begin{framed}

\[g(\epsilon) \frac{1}{1+\exp(-\epsilon)} \in (0,1) \quad\quad \text{logistische Funktion}\]
\[\frac{d_g(t)}{dt} = \frac{1}{(1+ \exp (-t))}^2 - \exp(-t) = \frac{1}{1+\exp (-t)} \left(1- \frac{1}{1+\exp (t)}\right)\]
\[g(t)(1-g(t))\]
\[\Rightarrow \frac{\delta h_\Theta(x)}{\delta \Theta_j} = h_\Theta(x)(1-h_\Theta(x)) \frac{\delta}{\delta \Theta_j} \Theta^T x\]
\[= h_\Theta(x)(1-H_\Theta(x))x_j\]
\begin{framed}
Kettenregel
\[h_\Theta(x) = g(\Theta^T x)\]
\[\underbrace{x}_{\substack{\in \mathbb{R}^n}} \underbrace{\mapsto}_{\substack{\text{innere Fkt.}}} \underbrace{\Theta^T}_{\substack{\in \mathbb{R}^n x}} \underbrace{\mapsto}_{\substack{\text{äußere Fkt.}}} \underbrace{g(\Theta^T x)}_{\substack{\in \mathbb{R}^n}}\]
\[\frac{\delta}{\delta \Theta_j} \Theta^T x = \frac{\delta}{\delta\Theta_j}(\Sigma^m_{i=0} \Theta_i x_i) = \frac{\delta}{\delta\Theta_j}(\Theta_1 x_1 + \Theta_1 x_1 + \dots + \Theta_n  x_n) = x_j\]
\end{framed}
\end{framed}


Wir wissen schon: 
\[ \frac{\delta l(\Theta)}{\delta \Theta_j} = \Sigma^m_{i=1}(\underbrace{y^{(i)}}_{\substack{\in \{0,1\}}} - \underbrace{\Theta(x^{(i)}))}_{\substack{\in (0,1)}} x^{(i)}_j\]
\[ \frac{\delta^2 l(\Theta)}{\delta \Theta_k \delta \Theta_j} = \frac{\delta}{\delta \Theta_k }\Sigma^m_{i=1} (y^{(i)} - h_\Theta(x^{(i)})) x_j^{(i)}\]
\[ =\underbrace{\frac{\delta}{\delta \Theta_k} (\Sigma_{i=1}^m y^{(i)} x_j^{(i)})}_{\substack{= 0, \text{hängt nicht von $\Theta$ ab}}} - \frac{\delta}{\delta \Theta_k} (\Sigma^m_{i=1} h_\Theta(x^{(i)}) x_j^{(i)})\]
\[ = \frac{\delta}{\delta \Theta_k} \Sigma^m_{i=1} h_\Theta (x^{(i)}) x_j^{(i)}\]
\[ = - \Sigma^m_{i=1} \frac{\delta}{\delta \Theta_k} (h_\Theta(x^{(i)}) x_j^{(i)})\]
\[ = - \Sigma^m_{i=1} x_j^{(i)} \frac{\delta}{\delta\Theta_k} h_\Theta(x^{(i)})\]
\[ = - \Sigma^m_{i=1} \underbrace{h_\Theta(x^{(i)})}_{\substack{> 0}} \underbrace{(1- h_\Theta(x^{(i)}))}_{\substack{> 0}} x_j^{(i)} x_k^{(i)}\]
Wir müssen zeigen, dass $v^T \quad H(\Theta)v \geq 0$ für alle $v \in \mathbb{R}^n$
\[ \underbrace{H(\Theta)}_{\substack{\in \mathbb{R}^{n \times n}}} = (H_{kj}(\Theta)) = \left( \frac{\delta^2 l(\Theta)}{\delta\Theta_k \delta\Theta_j}\right)\]

%Ich hoffe, dass es ab hier auch wirklich so weiter ging
$H(\Theta)$ ist symmetrisch, da
\[H_{kj}(\Theta) = - \Sigma^m_{i=1} h_\Theta (x^{(i)})(1-h_\Theta (x^{(i)})) x_j^{(i)} x_k^{(i)}\]
\[= - \Sigma^m_{i=1} h_\Theta(x^{(i)})(1-h_\Theta(x^{(i)})) x_k^{(i)}x_j^{(i)}\]
\[= H_{jk}(\Theta) = x_j\]
negativ semi-definit
\[H(\Theta) = - \Sigma^m_{i=1} h_\Theta(x^{(i)})(1-h_\Theta(x^{(i)})) \underbrace{x^{(i)}x^{(i)^T}}\]
\begin{center}
äußeres Produkt des i-ten Datenvektors mit sich selbst
\end{center}
Bsp. Äußeres Produkt:
\[x = \left( \begin{array}{ccc} x_1 \\ x_2 \end{array} \right), \quad x x^T = \left( \begin{array}{ccc} x_1 \\ x_2 \end{array} \right) (x_1 x_2) \]
\[= \left( \begin{array}{ccc} x_1x_1 \quad x_1x_2 \\ x_2x_1 \quad x_2x_2 \end{array} \right) = \left( \begin{array}{ccc} x_1^2 \quad x_1x_2 \\ x_2x_1 \quad x_2^2 \end{array} \right)\]
\[ = \left( \begin{array}{ccc} x_1^2 \quad x_1x_2 \\ x_1x_2 \quad x_2^2 \end{array} \right)\]
\begin{enumerate}[(1)]
\item äußeres Produkt ist symmetrische Matrix
\item äußeres Produkt ist semi-definit
\end{enumerate}
\[v^T (x x^T)v\]
\[= (v^T)(x^T v)\]
\[= (v^T)(v^T x)\]
\[(v^T x)^2 \geq 0\]
\begin{center}
$xx^T$ aus Abbildung $\mathbb{R}^n \mapsto \mathbb{R}^n$\\
$v \mapsto (xx^T)v = x(v^Tx)$
\end{center}

\subsection*{Statistische Modelle}
\begin{enumerate}[1.]
\item lineare Regression: \[p(y,x) = \frac{1}{\sqrt{2 \pi} \sigma} \exp(- \frac{\lVert y- \Theta^T x \lVert^2_2}{2 \sigma^2})\]
\item logistische Regression: \[P[y=1;x] = h_\Theta(x)\]
\[P[y=0;x] = 1-h_\Theta(x)\]
\end{enumerate}

$h_\Theta(x) = \frac{1}{1+\exp(-\Theta^T x)}$ benutzt logistische Funktion $g(z) = \frac{1}{1+\exp(-z)}$
1,2 sind konditionale Modelle, d.h. nur $y$ ist zufällig\\
$y \in \mathbb{R}$ (lineare Regression)\\
$y \in \{0,1\}$ (logistische Regression)\\
$\Rightarrow x$ frei wählbar, Messung von $y$ an der Stelle $x$ ist Zufallsexperiment

\begin{enumerate}
\item[3.] Gauss'sche Diskriminanzanalyse (Klassifikation)
\item[4.] Naive Bayes (Kontingenzanalyse)
\end{enumerate}

3,4 sind generative Modelle, d.h. sowohl $x$ als auch $y$ sind zufällig
\item Likelihood-Funktion der Gauss'schen Diskriminanzanalyse
\[L(\phi,\mu_0, \mu_1, \Sigma) = \prod^m_{i=1} p(x^{(i)},y^{(i)};\phi,\mu_0, \mu_1, \Sigma\]
\[\phi \in [0,1], \quad\quad \mu_0, \mu_1 \in \mathbb{R}^n, \quad\quad \Sigma \in \mathbb{R}^{n \times n} \text{positiv definit}\]
\[\text{Datenpunkte: } (x^{(1)},y^{(1)}, \dots , x^{(m)},y^{(m)})\]

\begin{framed}
Erinnerung: Likelihood-Funktion für logistische Regression:
\[L(\Theta) = \prod^n_{i=1} P[y^{(i)} | x^{(i)}, \Theta]\]
\end{framed}

\begin{framed}
Unterscheidung:
\begin{enumerate}[1.]
\item Zufallsvariable $x$ nimmt nur endlich viele Werte $\{1, \dots , k\}$ an. Dann kann $x = i$ eine Wahrscheinlichkeit $P[x = i]$ zuordnen.
\item Zufallsvariable $x$ nimmt Werte in $\mathbb{R}$ an. Dann kann man den Ereignissen $X \in [a,b]$ eine Wahrscheinlichkeit zuordnen $P[X \in [a,b]] = \int^b_a p(x) dx$. Dabei ist $p:\mathbb{R} \rightarrow \mathbb{R}$ eine Wahrscheinlichkeitsdichte, d.h.

\begin{enumerate}[(1.)]
\item $\int^\infty_{- \infty} p(x) dx = 1$
\item $p(x) \geq 0$
\end{enumerate}
\end{enumerate}
\end{framed}

\end{enumerate}